{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit\n",
    "from collections import defaultdict\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "\n",
    "DATA_TRAIN_PATH = 'train.csv' # path to train set\n",
    "DATA_TEST_PATH = 'test.csv'   # path to test set\n",
    "OUTPUT_PATH = 'out.csv'       # path to output\n",
    "\n",
    "init_train_y, init_train_x, ids_train = load_csv_data(DATA_TRAIN_PATH, sub_sample=False)\n",
    "_, init_test_x, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "init_train_y = (init_train_y + 1.0) * 0.5\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility methods\n",
    "When you add your own methods, please add a short description so that we all know what it does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cropped_of_rows_with_999_value(x, y):\n",
    "    \"\"\"Returns updated x and y such that all row which contained\n",
    "       at least one value = -999 are removed\"\"\"\n",
    "    for i in range(x.shape[1]):\n",
    "        idxes = (x[:, i] != -999)\n",
    "        x = x[idxes]\n",
    "        y = y[idxes]\n",
    "    return x, y\n",
    "\n",
    "def columns_with_999_value(x):\n",
    "    \"\"\"Returns the indices of all the columns which have at least one element = -999\"\"\"\n",
    "    indexes = []\n",
    "    for i in range(x.shape[1]):\n",
    "        if np.any(x[:, i] == -999):\n",
    "            indexes.append(i)\n",
    "    return indexes\n",
    "\n",
    "def column_mean_without_999(col):\n",
    "    \"\"\"Returns the mean of the vector without taking into account values = -999\"\"\"\n",
    "    return np.mean(col[col[:] != -999])\n",
    "\n",
    "def replace_999_with(col, val):\n",
    "    \"\"\"Replaces every element = -999 with the given value\"\"\"\n",
    "    col[col[:] == -999] = val\n",
    "\n",
    "def columns_with_low_corr(x, y):\n",
    "    \"\"\"Returns the indices of the columns which have a \n",
    "       'low' correlation to the output. (Experimental)\"\"\"\n",
    "    indexes = []\n",
    "    for i in range(x.shape[1]):\n",
    "        if np.abs(np.corrcoef(x[:, i], y)[0][1]) < 0.03:\n",
    "            indexes.append(i)\n",
    "    return indexes\n",
    "\n",
    "def cropped_of_columns(x, columns_indexes):\n",
    "    \"\"\"Returns a copy of x where the columns indexed by columns_indices are removed\"\"\"\n",
    "    return np.delete(x, columns_indexes, axis = 1)\n",
    "\n",
    "def get_999_indices_tuples(x):\n",
    "    indices = defaultdict(lambda: [], {})\n",
    "    for i in range(x.shape[0]):\n",
    "        indices[tuple(x[i] == -999)].append(i)\n",
    "    return indices\n",
    "\n",
    "def augment(x, n):\n",
    "    \"\"\"Returns x concatenated with x ** 2, ..., x ** n\"\"\"\n",
    "    return x if n == 1 else np.append(augment(x, n - 1), x ** n, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BasePhase(object):\n",
    "    \"\"\"Base class for phases\"\"\"\n",
    "    pass\n",
    "\n",
    "class Foreach(BasePhase):\n",
    "    def __init__(self, names, subpipe):\n",
    "        self.names = names\n",
    "        self.subpipe = subpipe\n",
    "        \n",
    "    def run(self, args):\n",
    "        items = [(name, args[name]) for name in self.names]\n",
    "        if len(items) > 0:\n",
    "            ret = {}\n",
    "            for i in range(len(items[0][1])):\n",
    "                substate = {name: val[i] for name, val in items}\n",
    "                run_pipeline(self.subpipe, substate)\n",
    "\n",
    "                for key, val in substate.items():\n",
    "                    if key in ret: ret[key].append(val)\n",
    "                    else: ret[key] = [val]\n",
    "\n",
    "            return ret\n",
    "\n",
    "def run_pipeline(pipeline, args):\n",
    "    for phase in pipeline:\n",
    "        ret = phase.run(args)\n",
    "        if ret is not None: args.update(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class InitPhase(BasePhase):\n",
    "    \"\"\"Copies the train and test set before executing next phase\"\"\"\n",
    "    def run(self, args):\n",
    "        ret = {}\n",
    "        ret[\"train_x\"] = np.copy(args[\"init_train_x\"])\n",
    "        ret[\"train_y\"] = np.copy(args[\"init_train_y\"])\n",
    "        if \"init_test_x\" in args:\n",
    "            ret[\"test_x\"] = np.copy(args[\"init_test_x\"])\n",
    "        return ret\n",
    "\n",
    "class StandardizePhase(BasePhase):\n",
    "    \"\"\"Standardize the dataset\"\"\"\n",
    "    def run(self, args):\n",
    "        ret = {}\n",
    "        ret[\"train_x\"], mean, var = standardize(args[\"train_x\"])\n",
    "        if \"test_x\" in args:\n",
    "            ret[\"test_x\"], _, _ = standardize(args[\"test_x\"], mean, var)\n",
    "        \n",
    "        return ret\n",
    "\n",
    "class AugmentPhase(BasePhase):\n",
    "    \"\"\"Augments the dataset to the given degree n\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        \n",
    "    def run(self, args):\n",
    "        ret = {}\n",
    "        ret[\"train_x\"] = augment(args[\"train_x\"], self.n)\n",
    "        if \"test_x\" in args:\n",
    "            ret[\"test_x\"] = augment(args[\"test_x\"], self.n)\n",
    "        \n",
    "        return ret\n",
    "\n",
    "class Replace999ByColumnMeanPhase(BasePhase):\n",
    "    \"\"\"Replaces cells = -999 with their respective column's mean\"\"\"\n",
    "    def run(self, args):\n",
    "        train_x = args[\"train_x\"]\n",
    "        for i in range(train_x.shape[1]):\n",
    "            col = train_x[:, i]\n",
    "            mean = column_mean_without_999(col)\n",
    "            replace_999_with(col, mean)\n",
    "        \n",
    "        if \"test_x\" in args:\n",
    "            test_x = args[\"test_x\"]\n",
    "            for i in range(train_x.shape[1]):\n",
    "                col = test_x[:, i]\n",
    "                mean = column_mean_without_999(col)\n",
    "                replace_999_with(col, mean)\n",
    "    \n",
    "class RemoveLowCorrelationFeaturesPhase(BasePhase):\n",
    "    \"\"\"Replaces cells = -999 with their respective column's mean\"\"\"\n",
    "    def run(self, args):\n",
    "        train_x, train_y = args[\"train_x\"], args[\"train_y\"]\n",
    "        indices = columns_with_low_corr(train_x, train_y)\n",
    "        train_x = cropped_of_columns(train_x, indices)\n",
    "        \n",
    "        if \"test_x\" in args:\n",
    "            test_x = cropped_of_columns(args[\"test_x\"], indices)\n",
    "            \n",
    "        return {\"train_x\": train_x, \"test_x\": test_x}\n",
    "\n",
    "class SplitUpon999Phase(BasePhase):\n",
    "    \"\"\"Splits the dataset according to some -999 criterion\"\"\"\n",
    "    def run(self, args):\n",
    "        train_x, train_y = args[\"train_x\"], args[\"train_y\"]\n",
    "        \n",
    "        ret = {\"train_row_indices\": [], \"train_x\": [], \"train_y\": []}\n",
    "        \n",
    "        if \"test_x\" in args:\n",
    "            test_x = args[\"test_x\"]\n",
    "            test_map = get_999_indices_tuples(test_x)\n",
    "            ret[\"test_row_indices\"] = []\n",
    "            ret[\"test_x\"] = []\n",
    "        else:\n",
    "            test_x = None\n",
    "        \n",
    "        for col_indices, row_indices in get_999_indices_tuples(train_x).items():\n",
    "            ret[\"train_row_indices\"].append(row_indices)\n",
    "            ret[\"train_x\"].append(np.delete(train_x[row_indices], np.where(col_indices), axis = 1))\n",
    "            ret[\"train_y\"].append(train_y[row_indices])\n",
    "            \n",
    "            if test_x is not None:\n",
    "                test_row_indices = test_map[col_indices]\n",
    "                ret[\"test_row_indices\"].append(test_row_indices)\n",
    "                ret[\"test_x\"].append(np.delete(test_x[test_row_indices], np.where(col_indices), axis = 1))\n",
    "        \n",
    "        return ret\n",
    "    \n",
    "class PlotFeatureOutputCorrelationPhase(BasePhase):\n",
    "    \"\"\"Plots feature to output correlation chart graph\"\"\"\n",
    "    def run(self, args):\n",
    "        train_x, train_y = args[\"train_x\"], args[\"train_y\"]\n",
    "        feature_count = train_x.shape[1]\n",
    "\n",
    "        corrs = np.zeros(feature_count)\n",
    "        for i in range(feature_count):\n",
    "            feature = train_x[:, i]\n",
    "            corrs[i] = np.corrcoef(feature, train_y)[0][1]\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        ax.bar(np.arange(feature_count), corrs)\n",
    "        \n",
    "class PlotGramMatrixPhase(BasePhase):\n",
    "    \"\"\"Plots the Gram matrix\"\"\"\n",
    "    def run(self, args):\n",
    "        train_x = args[\"train_x\"]\n",
    "        G = train_x.T @ train_x\n",
    "        plt.matshow(G, cmap = plt.cm.gray)\n",
    "\n",
    "init_pipeline = [\n",
    "    InitPhase(),\n",
    "    Replace999ByColumnMeanPhase(),\n",
    "    AugmentPhase(6),\n",
    "    PlotFeatureOutputCorrelationPhase(),\n",
    "    StandardizePhase()\n",
    "]\n",
    "\n",
    "test_pipeline = [\n",
    "    InitPhase(),\n",
    "    SplitUpon999Phase(),\n",
    "    Foreach([\"train_x\", \"train_y\", \"test_x\"], [\n",
    "        AugmentPhase(6),\n",
    "        StandardizePhase()\n",
    "    ])\n",
    "]\n",
    "\n",
    "state = {\n",
    "    \"init_train_x\": init_train_x, \n",
    "    \"init_train_y\": init_train_y, \n",
    "    \"init_test_x\": init_test_x,\n",
    "    \"ids_test\": ids_test\n",
    "}\n",
    "\n",
    "run_pipeline(test_pipeline, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Romain\\Programmation\\Python\\PCML16\\implementations.py:70: RuntimeWarning: overflow encountered in exp\n",
      "  return w, np.abs(np.sum(np.log(1 + np.exp(tx_w)) - y * tx_w)) / len(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 fold: (Train error: 0.6973612089891159, Test error: 0.6988489546629082)\n",
      "#1 fold: (Train error: 0.6787448124657427, Test error: 0.6826403570589618)\n",
      "#2 fold: (Train error: 0.727351029676611, Test error: 0.7237491190979564)\n",
      "#3 fold: (Train error: 0.7563816459165297, Test error: 0.7600422832980972)\n",
      "Cross Validation:\n",
      "#0 fold: (Train error: 0.9255232261357835, Test error: 0.9243491577335375)\n",
      "#1 fold: (Train error: 0.9327207759060745, Test error: 0.9366003062787136)\n",
      "#2 fold: (Train error: 0.9294027565084226, Test error: 0.923583460949464)\n",
      "#3 fold: (Train error: 0.935375191424196, Test error: 0.9376722817764166)\n",
      "Cross Validation:\n",
      "#0 fold: (Train error: 0.7577203158598508, Test error: 0.7601235973328997)\n",
      "#1 fold: (Train error: 0.6752859543557218, Test error: 0.6756654198514663)\n",
      "#2 fold: (Train error: 0.7540702191864983, Test error: 0.7490648885997723)\n",
      "#3 fold: (Train error: 0.7524620082759618, Test error: 0.7523174499918686)\n",
      "Cross Validation:\n",
      "#0 fold: (Train error: 0.8756398675097862, Test error: 0.8681120144534779)\n",
      "#1 fold: (Train error: 0.8810599217103282, Test error: 0.8879855465221319)\n",
      "#2 fold: (Train error: 0.8997289972899729, Test error: 0.8870822041553749)\n",
      "#3 fold: (Train error: 0.8747365251430292, Test error: 0.8699186991869918)\n",
      "Cross Validation:\n",
      "#0 fold: (Train error: 0.7077450700200058, Test error: 0.7088310945984567)\n",
      "#1 fold: (Train error: 0.703019910450605, Test error: 0.7035724492712203)\n",
      "#2 fold: (Train error: 0.7146613318090883, Test error: 0.7113460988853958)\n",
      "#3 fold: (Train error: 0.7228541488044203, Test error: 0.7194627036296085)\n",
      "Cross Validation:\n",
      "#0 fold: (Train error: 0.8984126984126984, Test error: 0.8973544973544973)\n",
      "#1 fold: (Train error: 0.8740740740740741, Test error: 0.8788359788359789)\n",
      "#2 fold: (Train error: 0.8922398589065256, Test error: 0.8809523809523809)\n",
      "#3 fold: (Train error: 0.854320987654321, Test error: 0.8359788359788359)\n"
     ]
    }
   ],
   "source": [
    "class CrossValidationPhase(BasePhase):\n",
    "    def __init__(self, k_fold, pred_subpipe):\n",
    "        self.k_fold = k_fold\n",
    "        self.pred_subpipe = pred_subpipe\n",
    "        \n",
    "    def _build_k_indices(self, num_row):\n",
    "        \"\"\"build k indices for k-fold.\"\"\"\n",
    "        interval = int(num_row / self.k_fold)\n",
    "        indices = np.random.permutation(num_row)\n",
    "        k_indices = [indices[k * interval: (k + 1) * interval] for k in range(self.k_fold)]\n",
    "        return np.array(k_indices)\n",
    "\n",
    "    def _cross_validation(self, y, tx, k_indices, k):\n",
    "        \"\"\"return the loss of ridge regression.\"\"\"\n",
    "        tmp_y = np.delete(y[k_indices], k, 0)\n",
    "        tmp_x = np.delete(tx[k_indices], k, 0)\n",
    "        train_y = tmp_y.reshape(tmp_y.shape[0] * tmp_y.shape[1])\n",
    "        train_x = tmp_x.reshape((tmp_x.shape[0] * tmp_x.shape[1], tmp_x.shape[2]))\n",
    "        test_y  = y[k_indices[k]]\n",
    "        test_x  = tx[k_indices[k]]\n",
    "\n",
    "        state = {\"train_x\": train_x, \"train_y\": train_y}\n",
    "        run_pipeline(self.pred_subpipe, state)\n",
    "        pred = state[\"pred\"]\n",
    "        \n",
    "        train_y_pred = pred.predict(train_x)\n",
    "        test_y_pred  = pred.predict(test_x)\n",
    "\n",
    "        train_y_err = np.count_nonzero(train_y_pred == train_y) / len(train_y)\n",
    "        test_y_err  = np.count_nonzero(test_y_pred  == test_y ) / len(test_y)\n",
    "\n",
    "        return train_y_err, test_y_err\n",
    "    \n",
    "    def run(self, args):\n",
    "        train_x, train_y = args[\"train_x\"], args[\"train_y\"]\n",
    "        k_fold = 4\n",
    "        k_indices = self._build_k_indices(len(train_y))\n",
    "        print(\"Cross Validation:\")\n",
    "        for k in range(k_fold):\n",
    "            train_err, test_err = self._cross_validation(train_y, train_x, k_indices, k)\n",
    "            print(\"#%s fold: (Train error: %s, Test error: %s)\" % (k, train_err, test_err))\n",
    "            \n",
    "class TrainBasicPredictorPhase(BasePhase):\n",
    "    \"\"\"Trains a predictor with the given train set\"\"\"\n",
    "    def __init__(self, gamma, max_iters):\n",
    "        self.gamma = gamma\n",
    "        self.max_iters = max_iters\n",
    "        \n",
    "    def run(self, args):\n",
    "        train_x, train_y = args[\"train_x\"], args[\"train_y\"]\n",
    "        w, _ = logistic_regression(train_y, train_x, np.zeros(train_x.shape[1]), self.max_iters, self.gamma)\n",
    "        return {\"pred\": Predictor(w)}\n",
    "    \n",
    "class Predictor(object):\n",
    "    def __init__(self, w):\n",
    "        self.w = w\n",
    "    \n",
    "    def predict(self, x):\n",
    "        y_pred = expit(x @ self.w)\n",
    "\n",
    "        y_pred[y_pred <  0.5] = 0\n",
    "        y_pred[y_pred >= 0.5] = 1\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "trainPredictPhase = TrainBasicPredictorPhase(3, 20)\n",
    "\n",
    "train_pipeline = [\n",
    "    CrossValidationPhase(4, [trainPredictPhase]),\n",
    "    trainPredictPhase\n",
    "]\n",
    "\n",
    "test_train_pipeline = [\n",
    "    Foreach([\"train_x\", \"train_y\"], train_pipeline)\n",
    "]\n",
    "\n",
    "run_pipeline(test_train_pipeline, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7512992923628036  for  68114  points\n",
      "0.9358419783332695  for  26123  points\n",
      "0.7022360753489633  for  73790  points\n",
      "0.8679160081282457  for  4429  points\n",
      "0.7270012288874282  for  69982  points\n",
      "0.8893149960327955  for  7562  points\n"
     ]
    }
   ],
   "source": [
    "class PredictionOnTrainSetPhase(BasePhase):\n",
    "    \"\"\"Predicts the output of the given dataset\"\"\"\n",
    "    def run(self, args):\n",
    "        train_x, train_y, pred = args[\"train_x\"], args[\"train_y\"], args[\"pred\"]\n",
    "        y_pred = pred.predict(train_x)\n",
    "        print(np.count_nonzero(y_pred == train_y) / len(train_y), \" for \", len(train_y), \" points\")\n",
    "\n",
    "class PredictionOnTestSetPhase(BasePhase):\n",
    "    def run(self, args):\n",
    "        test_x, pred = args[\"test_x\"], args[\"pred\"]\n",
    "        y_pred = pred.predict(test_x)\n",
    "        return {\"y_pred\": y_pred}\n",
    "        \n",
    "class AggregateBackPredictionsFrom999SplitPhase(BasePhase):\n",
    "    \"\"\"Aggregates predictions\"\"\"\n",
    "    def run(self, args):\n",
    "        y_preds, row_indices = args[\"y_pred\"], args[\"test_row_indices\"]\n",
    "        \n",
    "        ret = {}\n",
    "        \n",
    "        total = 0\n",
    "        for i in range(len(row_indices)):\n",
    "            total += len(row_indices[i])\n",
    "            \n",
    "        y_pred = np.zeros(total)\n",
    "        for i in range(len(row_indices)):\n",
    "            y_pred[row_indices[i]] = y_preds[i]\n",
    "            \n",
    "        return {\"y_pred\": y_pred}\n",
    "        \n",
    "class CreateSubmissionPhase(BasePhase):\n",
    "    \"\"\"Creates the CSV submission file from the given predictions\"\"\"\n",
    "    def run(self, args):\n",
    "        y_pred, ids_test = args[\"y_pred\"], args[\"ids_test\"]\n",
    "        create_csv_submission(ids_test, y_pred * 2 - 1, OUTPUT_PATH)\n",
    "        \n",
    "test_pred_pipeline = [\n",
    "    Foreach([\"train_x\", \"train_y\", \"test_x\", \"pred\"], [\n",
    "        PredictionOnTrainSetPhase(),\n",
    "        PredictionOnTestSetPhase()\n",
    "    ]),\n",
    "    AggregateBackPredictionsFrom999SplitPhase(),\n",
    "    CreateSubmissionPhase()\n",
    "]\n",
    "\n",
    "run_pipeline(test_pred_pipeline, state)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
