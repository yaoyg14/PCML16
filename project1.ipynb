{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import time\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training and test data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "\n",
    "DATA_TRAIN_PATH = 'train.csv' # path to train set\n",
    "DATA_TEST_PATH = 'test.csv'   # path to test set\n",
    "OUTPUT_PATH = 'out.csv'       # path to output\n",
    "\n",
    "init_train_y, init_train_x, ids_train = load_csv_data(DATA_TRAIN_PATH, sub_sample=False)\n",
    "_, init_test_x, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "init_train_y = (init_train_y + 1.0) * 0.5\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility methods\n",
    "Contains all the utility methods that we are using in our pipeline to analyze and transform data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cropped_of_rows_with_999_value(x, y):\n",
    "    \"\"\"Returns updated x and y such that all row which contained\n",
    "       at least one value = -999 are removed\"\"\"\n",
    "    for i in range(x.shape[1]):\n",
    "        idxes = (x[:, i] != -999)\n",
    "        x = x[idxes]\n",
    "        y = y[idxes]\n",
    "    return x, y\n",
    "\n",
    "def columns_with_999_value(x):\n",
    "    \"\"\"Returns the indices of all the columns which have at least one element = -999\"\"\"\n",
    "    indexes = []\n",
    "    for i in range(x.shape[1]):\n",
    "        if np.any(x[:, i] == -999):\n",
    "            indexes.append(i)\n",
    "    return indexes\n",
    "\n",
    "def column_mean_without_999(col):\n",
    "    \"\"\"Returns the mean of the vector without taking into account values = -999\"\"\"\n",
    "    return np.mean(col[col[:] != -999])\n",
    "\n",
    "def replace_999_with(col, val):\n",
    "    \"\"\"Replaces every element = -999 with the given value\"\"\"\n",
    "    col[col[:] == -999] = val\n",
    "\n",
    "def columns_with_low_corr(x, y, threshold):\n",
    "    \"\"\"Returns the indices of the columns which have a \n",
    "       low correlation to the output, indicated by threshold.\"\"\"\n",
    "    indexes = []\n",
    "    for i in range(x.shape[1]):\n",
    "        if np.abs(np.corrcoef(x[:, i], y)[0][1]) < threshold:\n",
    "            indexes.append(i)\n",
    "    return indexes\n",
    "\n",
    "def cropped_of_columns(x, columns_indexes):\n",
    "    \"\"\"Returns a copy of x where the columns indexed by columns_indices are removed\"\"\"\n",
    "    return np.delete(x, columns_indexes, axis = 1)\n",
    "\n",
    "def get_999_indices_tuples(x):\n",
    "    \"\"\"Returns a dictionnary where the key is a tuple of column indices which have a -999 element\n",
    "       and the value is the list of rows which all have -999 values at these columns\"\"\"\n",
    "    indices = defaultdict(lambda: [], {})\n",
    "    for i in range(x.shape[0]):\n",
    "        indices[tuple(x[i] == -999)].append(i)\n",
    "    return indices\n",
    "\n",
    "def powerize(x, degree):\n",
    "    \"\"\"Returns x concatenated with x ** 2, ..., x ** degree\"\"\"\n",
    "    return x if degree == 1 else np.append(powerize(x, degree - 1), x ** degree, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline definitions\n",
    "Contains the primary and common definitions of our pipeline system.\n",
    "* A Pipeline is a iterable of phases.\n",
    "* A Phase is an object which has a run method taking a dictionnary of arguments, and returning None or a dictionnary of outputs.\n",
    "* When running a pipeline on an initial state, every phase will update the state with their return values after their are done, and this state will be passed onto the next phase as its arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BasePhase(object):\n",
    "    \"\"\"Base class for phases\"\"\"\n",
    "    def debug(self, *args):\n",
    "        global debug\n",
    "        if debug:\n",
    "            print(\"  \\x1b[31m[Debug][%s]: \" % self.__class__.__name__, *args, \"\\x1b[0m\")\n",
    "\n",
    "class Foreach(BasePhase):\n",
    "    \"\"\"Utility phase which executes subpipe for each zipped argument in names.\n",
    "       The values referred by the names in 'names' must therefore be lists.\"\"\"\n",
    "    def __init__(self, names, subpipe):\n",
    "        self.names = names\n",
    "        self.subpipe = subpipe\n",
    "        \n",
    "    def run(self, args):\n",
    "        items = [(name, args[name]) for name in self.names]\n",
    "        if len(items) > 0:\n",
    "            ret = {}\n",
    "            for i in range(len(items[0][1])):\n",
    "                substate = {name: val[i] for name, val in items}\n",
    "                run_pipeline(self.subpipe, substate)\n",
    "\n",
    "                for key, val in substate.items():\n",
    "                    if key in ret: ret[key].append(val)\n",
    "                    else: ret[key] = [val]\n",
    "\n",
    "            return ret\n",
    "\n",
    "def run_pipeline(pipeline, args):\n",
    "    \"\"\"Runs the given pipeline.\"\"\"\n",
    "    for phase in pipeline:\n",
    "        ret = phase.run(args)\n",
    "        if ret is not None: args.update(ret)\n",
    "\n",
    "debug = True #Set to False to prevent debug messages from being displayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and preprocessing phases\n",
    "This part contains the definitions of all the methods that we tried for analysing and preprocessing data.\n",
    "\n",
    "At the end of it is also defined the preprocessing pipeline that we used to get our top score, which uses a subset of all our methods. Check the report for an explanation of our process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class InitPhase(BasePhase):\n",
    "    \"\"\"Copies the train and test set before executing next phase.\n",
    "       This phase should be done at the start of the pipeline to prevent\n",
    "       the next phases to transform directly the original datasets\"\"\"\n",
    "    def run(self, args):\n",
    "        ret = {}\n",
    "        ret[\"train_x\"] = np.copy(args[\"init_train_x\"])\n",
    "        ret[\"train_y\"] = np.copy(args[\"init_train_y\"])\n",
    "        ret[\"test_x\"]  = np.copy(args[\"init_test_x\"])\n",
    "        return ret\n",
    "\n",
    "class StandardizePhase(BasePhase):\n",
    "    \"\"\"Standardizes the datasets\"\"\"\n",
    "    def run(self, args):\n",
    "        ret = {}\n",
    "        ret[\"train_x\"], mean, var = standardize(args[\"train_x\"])\n",
    "        ret[\"test_x\"], _, _ = standardize(args[\"test_x\"], mean, var)\n",
    "        return ret\n",
    "\n",
    "class InvertPhase(BasePhase):\n",
    "    \"\"\"Augments the datasets with 1/x for each feature x (if x == 0, we set 1/x == 0)\"\"\"\n",
    "    def run(self, args):\n",
    "        ret = {}\n",
    "        datasets = [\"train_x\", \"test_x\"]\n",
    "        for name in datasets:\n",
    "            cpy = np.copy(args[name])\n",
    "            cpy[cpy == 0] = float('+inf')\n",
    "            ret[name] = np.append(args[name], 1.0 / cpy, axis = 1)\n",
    "        return ret\n",
    "    \n",
    "class PowerizePhase(BasePhase):\n",
    "    \"\"\"Augments the datasets with the features raised to the power 2, ..., n, as well 1/2, 1/3 and 1/4.\n",
    "       n is computed from degree_fun, which must be a function of the number of samples.\"\"\"\n",
    "    def __init__(self, degree_fun):\n",
    "        self.degree_fun = degree_fun\n",
    "        \n",
    "    def run(self, args):\n",
    "        ret = {}\n",
    "        sample_count = len(args[\"train_y\"])\n",
    "        \n",
    "        degree = max(int(self.degree_fun(sample_count)), 1)\n",
    "        self.debug(\"Augmenting\", sample_count, \"samples up to degree\", degree)\n",
    "        \n",
    "        datasets = [\"train_x\", \"test_x\"]\n",
    "        for name in datasets:\n",
    "            absolute = np.abs(args[name]) #Doesn't change anything that the values are abs'd\n",
    "            ret[name] = np.concatenate(\n",
    "                (powerize(absolute, degree), \n",
    "                 absolute ** (1. / 2), \n",
    "                 absolute ** (1. / 3), \n",
    "                 absolute ** (1. / 4)), \n",
    "                axis = 1)\n",
    "        \n",
    "        return ret\n",
    "\n",
    "class Replace999ByColumnMeanPhase(BasePhase):\n",
    "    \"\"\"Replaces cells = -999 with their respective column's mean\"\"\"\n",
    "    def run(self, args):\n",
    "        datasets = [args[\"train_x\"], args[\"test_x\"]]\n",
    "        for dataset in datasets:\n",
    "            for i in range(dataset.shape[1]):\n",
    "                col = dataset[:, i]\n",
    "                mean = column_mean_without_999(col)\n",
    "                replace_999_with(col, mean)\n",
    "                \n",
    "class RemoveLowCorrelationFeaturesPhase(BasePhase):\n",
    "    \"\"\"Removes the features which have a low correlation with the output.\n",
    "       The 'low' correlation threshold is computed from threshold_fun, which must\n",
    "       be a function of the number of samples\"\"\"\n",
    "    def __init__(self, threshold_fun):\n",
    "        self.threshold_fun = threshold_fun\n",
    "\n",
    "    def run(self, args):\n",
    "        train_x, train_y = args[\"train_x\"], args[\"train_y\"]\n",
    "        threshold = self.threshold_fun(len(train_y))\n",
    "        indices = columns_with_low_corr(train_x, train_y, threshold)\n",
    "        train_x = cropped_of_columns(train_x, indices)\n",
    "        self.debug(\"Removed\", np.count_nonzero(indices), \"features according to threshold =\", threshold)\n",
    "        \n",
    "        if \"test_x\" in args:\n",
    "            test_x = cropped_of_columns(args[\"test_x\"], indices)\n",
    "            \n",
    "        return {\"train_x\": train_x, \"test_x\": test_x}\n",
    "\n",
    "class SplitUpon999Phase(BasePhase):\n",
    "    \"\"\"Splits the dataset into 6 subsets. See report for details\"\"\"\n",
    "    def run(self, args):\n",
    "        train_x, train_y, test_x = args[\"train_x\"], args[\"train_y\"], args[\"test_x\"]\n",
    "        \n",
    "        ret = {\"train_row_indices\": [], \"test_row_indices\": [], \n",
    "               \"train_x\": [], \"train_y\": [], \"test_x\": []}\n",
    "        \n",
    "        train_map = get_999_indices_tuples(train_x)\n",
    "        test_map  = get_999_indices_tuples(test_x)\n",
    "        \n",
    "        for col_indices, row_indices in train_map.items():\n",
    "            ret[\"train_row_indices\"].append(row_indices)\n",
    "            ret[\"train_x\"].append(np.delete(train_x[row_indices], np.where(col_indices), axis = 1))\n",
    "            ret[\"train_y\"].append(train_y[row_indices])\n",
    "\n",
    "            test_row_indices = test_map[col_indices]\n",
    "            ret[\"test_row_indices\"].append(test_row_indices)\n",
    "            ret[\"test_x\"].append(np.delete(test_x[test_row_indices], np.where(col_indices), axis = 1))\n",
    "        \n",
    "        return ret\n",
    "    \n",
    "class PlotFeatureOutputCorrelationPhase(BasePhase):\n",
    "    \"\"\"Plots feature-to-output correlation chart graph\"\"\"\n",
    "    def __init__(self, caption = \"\"):\n",
    "        self.caption = caption\n",
    "    \n",
    "    def run(self, args):\n",
    "        train_x, train_y = args[\"train_x\"], args[\"train_y\"]\n",
    "        feature_count = train_x.shape[1]\n",
    "\n",
    "        corrs = np.zeros(feature_count)\n",
    "        for i in range(feature_count):\n",
    "            feature = train_x[:, i]\n",
    "            corrs[i] = np.corrcoef(feature, train_y)[0][1]\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.suptitle(self.caption)\n",
    "        ax.bar(np.arange(feature_count), corrs)\n",
    "\n",
    "class PlotGramMatrixPhase(BasePhase):\n",
    "    \"\"\"Plots the Gram matrix of the train set\"\"\"\n",
    "    def run(self, args):\n",
    "        train_x = args[\"train_x\"]\n",
    "        G = train_x.T @ train_x\n",
    "        plt.matshow(G, cmap = plt.cm.gray)\n",
    "\n",
    "# The pipline used for our final output\n",
    "init_pipeline = [\n",
    "    InitPhase(),\n",
    "    SplitUpon999Phase(),\n",
    "    Foreach([\"train_x\", \"train_y\", \"test_x\"], [\n",
    "        InvertPhase(),\n",
    "        PowerizePhase(lambda sample_count: 14 * sample_count / 40000),\n",
    "        PlotFeatureOutputCorrelationPhase(\"Correlation of each feature with the output\"),\n",
    "        RemoveLowCorrelationFeaturesPhase(lambda sample_count: 0.02),\n",
    "        PlotFeatureOutputCorrelationPhase(\"After features with low correlation are removed\"),\n",
    "        StandardizePhase()\n",
    "    ])\n",
    "]\n",
    "\n",
    "state = {\n",
    "    \"init_train_x\": init_train_x, \n",
    "    \"init_train_y\": init_train_y, \n",
    "    \"init_test_x\": init_test_x,\n",
    "    \"ids_test\": ids_test\n",
    "}\n",
    "\n",
    "run_pipeline(init_pipeline, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training phases\n",
    "In this part can be found our phases which deal with training, as well as our other data structures. Again, you can check at the end the training pipeline that we used to get our top score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CrossValidationPhase(BasePhase):\n",
    "    \"\"\"Performs k_fold cross validation\"\"\"\n",
    "    def __init__(self, k_fold, pred_subpipe):\n",
    "        self.k_fold = k_fold\n",
    "        self.pred_subpipe = pred_subpipe\n",
    "        \n",
    "    def _build_k_indices(self, num_row):\n",
    "        \"\"\"build k indices for k-fold.\"\"\"\n",
    "        interval = int(num_row / self.k_fold)\n",
    "        indices = np.random.permutation(num_row)\n",
    "        k_indices = [indices[k * interval: (k + 1) * interval] for k in range(self.k_fold)]\n",
    "        return np.array(k_indices)\n",
    "\n",
    "    def _cross_validation(self, y, tx, k_indices, k, args):\n",
    "        \"\"\"return the loss of ridge regression.\"\"\"\n",
    "        tmp_y = np.delete(y[k_indices], k, 0)\n",
    "        tmp_x = np.delete(tx[k_indices], k, 0)\n",
    "        train_y = tmp_y.reshape(tmp_y.shape[0] * tmp_y.shape[1])\n",
    "        train_x = tmp_x.reshape((tmp_x.shape[0] * tmp_x.shape[1], tmp_x.shape[2]))\n",
    "        test_y  = y[k_indices[k]]\n",
    "        test_x  = tx[k_indices[k]]\n",
    "\n",
    "        state = dict(args)\n",
    "        state.update({\"train_x\": train_x, \"train_y\": train_y})\n",
    "        run_pipeline(self.pred_subpipe, state)\n",
    "        pred = state[\"pred\"]\n",
    "        \n",
    "        train_y_pred = pred.predict(train_x)\n",
    "        test_y_pred  = pred.predict(test_x)\n",
    "\n",
    "        train_score = np.count_nonzero(train_y_pred == train_y) / len(train_y)\n",
    "        test_score  = np.count_nonzero(test_y_pred  == test_y ) / len(test_y)\n",
    "\n",
    "        return train_score, test_score\n",
    "    \n",
    "    def run(self, args):\n",
    "        train_x, train_y = args[\"train_x\"], args[\"train_y\"]\n",
    "        k_indices = self._build_k_indices(len(train_y))\n",
    "        \n",
    "        print(\"Cross Validation:\")\n",
    "        \n",
    "        total_train_score, total_test_score = 0, 0\n",
    "        for k in range(self.k_fold):\n",
    "            train_score, test_score = self._cross_validation(train_y, train_x, k_indices, k, args)\n",
    "            total_train_score += train_score\n",
    "            total_test_score += test_score\n",
    "            print(\"#%s fold: (Train score: %s%%, Test score: %s%%)\" % (k, train_score * 100, test_score * 100))\n",
    "        \n",
    "        print(\"Mean of train score: %s; Mean of test score: %s\" \n",
    "                  % (total_train_score * 100 / self.k_fold, total_test_score * 100 / self.k_fold))\n",
    "\n",
    "class TrainPredictorPhase(BasePhase):\n",
    "    \"\"\"Trains a predictor from the train set\"\"\"\n",
    "    def __init__(self, manual_copy = False):\n",
    "        self.manual_copy = manual_copy\n",
    "        \n",
    "    def run(self, args):\n",
    "        train_x, train_y, params = args[\"train_x\"], args[\"train_y\"], args[\"params\"]\n",
    "        \n",
    "        if self.manual_copy: #Solves a heavy slowdown in the logistic_regression\n",
    "            tmp_y = np.empty((0))\n",
    "            tmp_x = np.empty((0, train_x.shape[1]))\n",
    "            train_y = np.append(tmp_y, train_y, axis = 0)\n",
    "            train_x = np.append(tmp_x, train_x, axis = 0)\n",
    "            \n",
    "        self.debug(u\"Training predictor with\", train_x.shape[0], \"samples and\", train_x.shape[1], \"features.\", \n",
    "                   \"(max_iters=%s, \\u0393=%s, \\u03BB=%s)\" % (params.max_iters, params.gamma, params.lambda_))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        w, _ = reg_logistic_regression(train_y, train_x, params.lambda_, np.zeros(train_x.shape[1]), \n",
    "                                   params.max_iters, params.gamma)\n",
    "        self.debug(\"Done in\", time.time() - start_time, \"seconds\")\n",
    "        return {\"pred\": Predictor(w)}\n",
    "\n",
    "class TrainingParameters(object):\n",
    "    \"\"\"Structure which contains parameters for a predictor trainer\"\"\"\n",
    "    def __init__(self, max_iters = 200, gamma = 0.1, lambda_ = 0.01):\n",
    "        self.max_iters = max_iters\n",
    "        self.gamma = gamma\n",
    "        self.lambda_ = lambda_\n",
    "        \n",
    "class Predictor(object):\n",
    "    \"\"\"Small wrapper of 'w' with a predict method\"\"\"\n",
    "    def __init__(self, w):\n",
    "        self.w = w\n",
    "    \n",
    "    def predict(self, x):\n",
    "        y_pred = sigmoid(x @ self.w)\n",
    "\n",
    "        y_pred[y_pred <  0.5] = 0\n",
    "        y_pred[y_pred >= 0.5] = 1\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "#The training pipeline\n",
    "test_train_pipeline = [\n",
    "    Foreach([\"train_x\", \"train_y\", \"params\"], [\n",
    "        #CrossValidationPhase(4, [TrainPredictorPhase(manual_copy = False)]),\n",
    "        TrainPredictorPhase(manual_copy = True)\n",
    "    ])\n",
    "]\n",
    "\n",
    "state.update({\n",
    "    \"params\": [\n",
    "        TrainingParameters(max_iters = 2000, gamma = 0.1, lambda_ = 0.01),\n",
    "        TrainingParameters(max_iters = 2000, gamma = 0.1, lambda_ = 0.01),\n",
    "        TrainingParameters(max_iters = 2000, gamma = 0.1, lambda_ = 0.01),\n",
    "        TrainingParameters(max_iters = 2000, gamma = 0.1, lambda_ = 0.01),\n",
    "        TrainingParameters(max_iters = 2000, gamma = 0.1, lambda_ = 0.01),\n",
    "        TrainingParameters(max_iters = 2000, gamma = 0.1, lambda_ = 0.01),\n",
    "    ]\n",
    "})\n",
    "\n",
    "run_pipeline(test_train_pipeline, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction phases\n",
    "This part contains the phases responsible for the final prediction and CSV generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_train_error = 0\n",
    "total_train_samples = 0\n",
    "\n",
    "class PredictionOnTrainSetPhase(BasePhase):\n",
    "    \"\"\"Predicts the output of the given dataset on the train set\n",
    "       and prints its score\"\"\"\n",
    "    def run(self, args):\n",
    "        train_x, train_y, pred = args[\"train_x\"], args[\"train_y\"], args[\"pred\"]\n",
    "        y_pred = pred.predict(train_x)\n",
    "        global total_train_error\n",
    "        global total_train_samples\n",
    "        total_train_error += np.count_nonzero(y_pred == train_y)\n",
    "        total_train_samples += len(train_y)\n",
    "        print(np.count_nonzero(y_pred == train_y) / len(train_y), \" for \", len(train_y), \" points\")\n",
    "\n",
    "class PredictionOnTestSetPhase(BasePhase):\n",
    "    \"\"\"Runs the predictor on the test set and outputs a prediction vector\"\"\"\n",
    "    def run(self, args):\n",
    "        test_x, pred = args[\"test_x\"], args[\"pred\"]\n",
    "        y_pred = pred.predict(test_x)\n",
    "        return {\"y_pred\": y_pred}\n",
    "        \n",
    "class AggregateBackPredictionsFrom999SplitPhase(BasePhase):\n",
    "    \"\"\"Aggregates predictions back from the 6 predictors \n",
    "       into a single final prediction vector\"\"\"\n",
    "    def run(self, args):\n",
    "        y_preds, row_indices = args[\"y_pred\"], args[\"test_row_indices\"]\n",
    "        \n",
    "        ret = {}\n",
    "        \n",
    "        total = 0\n",
    "        for i in range(len(row_indices)):\n",
    "            total += len(row_indices[i])\n",
    "            \n",
    "        y_pred = np.empty(total)\n",
    "        for i in range(len(row_indices)):\n",
    "            y_pred[row_indices[i]] = y_preds[i]\n",
    "            \n",
    "        return {\"y_pred\": y_pred}\n",
    "        \n",
    "class CreateSubmissionPhase(BasePhase):\n",
    "    \"\"\"Creates the CSV submission file from the given predictions\"\"\"\n",
    "    def run(self, args):\n",
    "        y_pred, ids_test = args[\"y_pred\"], args[\"ids_test\"]\n",
    "        create_csv_submission(ids_test, y_pred * 2 - 1, OUTPUT_PATH)\n",
    "\n",
    "#The test prediction pipeline\n",
    "test_pred_pipeline = [\n",
    "    Foreach([\"train_x\", \"train_y\", \"test_x\", \"pred\"], [\n",
    "        PredictionOnTrainSetPhase(),\n",
    "        PredictionOnTestSetPhase()\n",
    "    ]),\n",
    "    AggregateBackPredictionsFrom999SplitPhase(),\n",
    "    CreateSubmissionPhase()\n",
    "]\n",
    "\n",
    "run_pipeline(test_pred_pipeline, state)\n",
    "\n",
    "print(\"Total: \", total_train_error / total_train_samples)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
