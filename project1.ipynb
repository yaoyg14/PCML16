{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit\n",
    "from collections import defaultdict\n",
    "import time\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "\n",
    "DATA_TRAIN_PATH = 'train.csv' # path to train set\n",
    "DATA_TEST_PATH = 'test.csv'   # path to test set\n",
    "OUTPUT_PATH = 'out.csv'       # path to output\n",
    "\n",
    "init_train_y, init_train_x, ids_train = load_csv_data(DATA_TRAIN_PATH, sub_sample=False)\n",
    "_, init_test_x, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "init_train_y = (init_train_y + 1.0) * 0.5\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility methods\n",
    "When you add your own methods, please add a short description so that we all know what it does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cropped_of_rows_with_999_value(x, y):\n",
    "    \"\"\"Returns updated x and y such that all row which contained\n",
    "       at least one value = -999 are removed\"\"\"\n",
    "    for i in range(x.shape[1]):\n",
    "        idxes = (x[:, i] != -999)\n",
    "        x = x[idxes]\n",
    "        y = y[idxes]\n",
    "    return x, y\n",
    "\n",
    "def columns_with_999_value(x):\n",
    "    \"\"\"Returns the indices of all the columns which have at least one element = -999\"\"\"\n",
    "    indexes = []\n",
    "    for i in range(x.shape[1]):\n",
    "        if np.any(x[:, i] == -999):\n",
    "            indexes.append(i)\n",
    "    return indexes\n",
    "\n",
    "def column_mean_without_999(col):\n",
    "    \"\"\"Returns the mean of the vector without taking into account values = -999\"\"\"\n",
    "    return np.mean(col[col[:] != -999])\n",
    "\n",
    "def replace_999_with(col, val):\n",
    "    \"\"\"Replaces every element = -999 with the given value\"\"\"\n",
    "    col[col[:] == -999] = val\n",
    "\n",
    "def columns_with_low_corr(x, y, threshold):\n",
    "    \"\"\"Returns the indices of the columns which have a \n",
    "       'low' correlation to the output. (Experimental)\"\"\"\n",
    "    indexes = []\n",
    "    for i in range(x.shape[1]):\n",
    "        if np.abs(np.corrcoef(x[:, i], y)[0][1]) < threshold:\n",
    "            indexes.append(i)\n",
    "    return indexes\n",
    "\n",
    "def cropped_of_columns(x, columns_indexes):\n",
    "    \"\"\"Returns a copy of x where the columns indexed by columns_indices are removed\"\"\"\n",
    "    return np.delete(x, columns_indexes, axis = 1)\n",
    "\n",
    "def get_999_indices_tuples(x):\n",
    "    indices = defaultdict(lambda: [], {})\n",
    "    for i in range(x.shape[0]):\n",
    "        indices[tuple(x[i] == -999)].append(i)\n",
    "    return indices\n",
    "\n",
    "def powerize(x, degree):\n",
    "    \"\"\"Returns x concatenated with x ** 2, ..., x ** degree\"\"\"\n",
    "    return x if degree == 1 else np.append(powerize(x, degree - 1), x ** degree, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BasePhase(object):\n",
    "    \"\"\"Base class for phases\"\"\"\n",
    "    def debug(self, *args):\n",
    "        global debug\n",
    "        if debug:\n",
    "            print(\"  \\x1b[31m[Debug][%s]: \" % self.__class__.__name__, *args, \"\\x1b[0m\")\n",
    "\n",
    "class Foreach(BasePhase):\n",
    "    def __init__(self, names, subpipe):\n",
    "        self.names = names\n",
    "        self.subpipe = subpipe\n",
    "        \n",
    "    def run(self, args):\n",
    "        items = [(name, args[name]) for name in self.names]\n",
    "        if len(items) > 0:\n",
    "            ret = {}\n",
    "            for i in range(len(items[0][1])):\n",
    "                substate = {name: val[i] for name, val in items}\n",
    "                run_pipeline(self.subpipe, substate)\n",
    "\n",
    "                for key, val in substate.items():\n",
    "                    if key in ret: ret[key].append(val)\n",
    "                    else: ret[key] = [val]\n",
    "\n",
    "            return ret\n",
    "\n",
    "def run_pipeline(pipeline, args):\n",
    "    for phase in pipeline:\n",
    "        ret = phase.run(args)\n",
    "        if ret is not None: args.update(ret)\n",
    "\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class InitPhase(BasePhase):\n",
    "    \"\"\"Copies the train and test set before executing next phase\"\"\"\n",
    "    def run(self, args):\n",
    "        ret = {}\n",
    "        ret[\"train_x\"] = np.copy(args[\"init_train_x\"])\n",
    "        ret[\"train_y\"] = np.copy(args[\"init_train_y\"])\n",
    "        if \"init_test_x\" in args:\n",
    "            ret[\"test_x\"] = np.copy(args[\"init_test_x\"])\n",
    "        return ret\n",
    "\n",
    "class StandardizePhase(BasePhase):\n",
    "    \"\"\"Standardize the dataset\"\"\"\n",
    "    def run(self, args):\n",
    "        ret = {}\n",
    "        ret[\"train_x\"], mean, var = standardize(args[\"train_x\"])\n",
    "        if \"test_x\" in args:\n",
    "            ret[\"test_x\"], _, _ = standardize(args[\"test_x\"])\n",
    "        \n",
    "        return ret\n",
    "\n",
    "class InvertPhase(BasePhase):\n",
    "    def run(self, args):\n",
    "        ret = {}\n",
    "        datasets = [\"train_x\"] + ([\"test_x\"] if \"test_x\" in args else [])\n",
    "        for name in datasets:\n",
    "            cpy = np.copy(args[name])\n",
    "            cpy[cpy == 0] = float('+inf')\n",
    "            ret[name] = np.append(args[name], 1.0 / cpy, axis = 1)\n",
    "        return ret\n",
    "    \n",
    "class PowerizePhase(BasePhase):\n",
    "    \"\"\"Augments the dataset with the features raised to the power 2, ..., n, as well 1/2 and 1/3\"\"\"\n",
    "    def __init__(self, degree_fun):\n",
    "        self.degree_fun = degree_fun\n",
    "        \n",
    "    def run(self, args):\n",
    "        ret = {}\n",
    "        sample_count = len(args[\"train_y\"])\n",
    "        \n",
    "        degree = max(int(self.degree_fun(sample_count)), 1)\n",
    "        self.debug(\"Augmenting\", sample_count, \"samples up to degree\", degree)\n",
    "        \n",
    "        datasets = [\"train_x\"] + ([\"test_x\"] if \"test_x\" in args else [])\n",
    "        for name in datasets:\n",
    "            absolute = np.abs(args[name]) #Doesn't change anything that the values are abs'd\n",
    "            ret[name] = np.concatenate((powerize(absolute, degree), absolute ** (1. / 2), absolute ** (1. / 3)), axis = 1)\n",
    "        \n",
    "        return ret\n",
    "\n",
    "class Replace999ByColumnMeanPhase(BasePhase):\n",
    "    \"\"\"Replaces cells = -999 with their respective column's mean\"\"\"\n",
    "    def run(self, args):\n",
    "        train_x = args[\"train_x\"]\n",
    "        for i in range(train_x.shape[1]):\n",
    "            col = train_x[:, i]\n",
    "            mean = column_mean_without_999(col)\n",
    "            replace_999_with(col, mean)\n",
    "        \n",
    "        if \"test_x\" in args:\n",
    "            test_x = args[\"test_x\"]\n",
    "            for i in range(train_x.shape[1]):\n",
    "                col = test_x[:, i]\n",
    "                mean = column_mean_without_999(col)\n",
    "                replace_999_with(col, mean)\n",
    "    \n",
    "class RemoveLowCorrelationFeaturesPhase(BasePhase):\n",
    "    \"\"\"Replaces cells = -999 with their respective column's mean\"\"\"\n",
    "    def __init__(self, threshold_fun):\n",
    "        self.threshold_fun = threshold_fun\n",
    "\n",
    "    def run(self, args):\n",
    "        train_x, train_y = args[\"train_x\"], args[\"train_y\"]\n",
    "        threshold = self.threshold_fun(len(train_y))\n",
    "        indices = columns_with_low_corr(train_x, train_y, threshold)\n",
    "        train_x = cropped_of_columns(train_x, indices)\n",
    "        self.debug(\"Removed\", np.count_nonzero(indices), \"features according to threshold =\", threshold)\n",
    "        \n",
    "        if \"test_x\" in args:\n",
    "            test_x = cropped_of_columns(args[\"test_x\"], indices)\n",
    "            \n",
    "        return {\"train_x\": train_x, \"test_x\": test_x}\n",
    "\n",
    "class SplitUpon999Phase(BasePhase):\n",
    "    \"\"\"Splits the dataset according to some -999 criterion\"\"\"\n",
    "    def run(self, args):\n",
    "        train_x, train_y = args[\"train_x\"], args[\"train_y\"]\n",
    "        \n",
    "        ret = {\"train_row_indices\": [], \"train_x\": [], \"train_y\": []}\n",
    "        \n",
    "        if \"test_x\" in args:\n",
    "            test_x = args[\"test_x\"]\n",
    "            test_map = get_999_indices_tuples(test_x)\n",
    "            ret[\"test_row_indices\"] = []\n",
    "            ret[\"test_x\"] = []\n",
    "        else:\n",
    "            test_x = None\n",
    "        \n",
    "        for col_indices, row_indices in get_999_indices_tuples(train_x).items():\n",
    "            ret[\"train_row_indices\"].append(row_indices)\n",
    "            ret[\"train_x\"].append(np.delete(train_x[row_indices], np.where(col_indices), axis = 1))\n",
    "            ret[\"train_y\"].append(train_y[row_indices])\n",
    "            \n",
    "            if test_x is not None:\n",
    "                test_row_indices = test_map[col_indices]\n",
    "                ret[\"test_row_indices\"].append(test_row_indices)\n",
    "                ret[\"test_x\"].append(np.delete(test_x[test_row_indices], np.where(col_indices), axis = 1))\n",
    "        \n",
    "        return ret\n",
    "    \n",
    "class PlotFeatureOutputCorrelationPhase(BasePhase):\n",
    "    \"\"\"Plots feature to output correlation chart graph\"\"\"\n",
    "    def __init__(self, caption = \"\"):\n",
    "        self.caption = caption\n",
    "    \n",
    "    def run(self, args):\n",
    "        train_x, train_y = args[\"train_x\"], args[\"train_y\"]\n",
    "        feature_count = train_x.shape[1]\n",
    "\n",
    "        corrs = np.zeros(feature_count)\n",
    "        for i in range(feature_count):\n",
    "            feature = train_x[:, i]\n",
    "            corrs[i] = np.corrcoef(feature, train_y)[0][1]\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.suptitle(self.caption)\n",
    "        ax.bar(np.arange(feature_count), corrs)\n",
    "        \n",
    "class PlotGramMatrixPhase(BasePhase):\n",
    "    \"\"\"Plots the Gram matrix\"\"\"\n",
    "    def run(self, args):\n",
    "        train_x = args[\"train_x\"]\n",
    "        G = train_x.T @ train_x\n",
    "        plt.matshow(G, cmap = plt.cm.gray)\n",
    "\n",
    "init_pipeline = [\n",
    "    InitPhase(),\n",
    "    SplitUpon999Phase(),\n",
    "    Foreach([\"train_x\", \"train_y\", \"test_x\"], [\n",
    "        InvertPhase(),\n",
    "        PowerizePhase(lambda sample_count: 15 * sample_count / 40000),\n",
    "        PlotFeatureOutputCorrelationPhase(\"Correlation of each feature with the output\"),\n",
    "        RemoveLowCorrelationFeaturesPhase(lambda sample_count: 0.04),\n",
    "        PlotFeatureOutputCorrelationPhase(\"After features with low correlation are removed\"),\n",
    "        StandardizePhase()\n",
    "    ])\n",
    "]\n",
    "\n",
    "state = {\n",
    "    \"init_train_x\": init_train_x, \n",
    "    \"init_train_y\": init_train_y, \n",
    "    \"init_test_x\": init_test_x,\n",
    "    \"ids_test\": ids_test\n",
    "}\n",
    "\n",
    "run_pipeline(init_pipeline, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CrossValidationPhase(BasePhase):\n",
    "    def __init__(self, k_fold, pred_subpipe):\n",
    "        self.k_fold = k_fold\n",
    "        self.pred_subpipe = pred_subpipe\n",
    "        \n",
    "    def _build_k_indices(self, num_row):\n",
    "        \"\"\"build k indices for k-fold.\"\"\"\n",
    "        interval = int(num_row / self.k_fold)\n",
    "        indices = np.random.permutation(num_row)\n",
    "        k_indices = [indices[k * interval: (k + 1) * interval] for k in range(self.k_fold)]\n",
    "        return np.array(k_indices)\n",
    "\n",
    "    def _cross_validation(self, y, tx, k_indices, k):\n",
    "        \"\"\"return the loss of ridge regression.\"\"\"\n",
    "        tmp_y = np.delete(y[k_indices], k, 0)\n",
    "        tmp_x = np.delete(tx[k_indices], k, 0)\n",
    "        train_y = tmp_y.reshape(tmp_y.shape[0] * tmp_y.shape[1])\n",
    "        train_x = tmp_x.reshape((tmp_x.shape[0] * tmp_x.shape[1], tmp_x.shape[2]))\n",
    "        test_y  = y[k_indices[k]]\n",
    "        test_x  = tx[k_indices[k]]\n",
    "\n",
    "        state = {\"train_x\": train_x, \"train_y\": train_y}\n",
    "        run_pipeline(self.pred_subpipe, state)\n",
    "        pred = state[\"pred\"]\n",
    "        \n",
    "        train_y_pred = pred.predict(train_x)\n",
    "        test_y_pred  = pred.predict(test_x)\n",
    "\n",
    "        train_y_err = np.count_nonzero(train_y_pred == train_y) / len(train_y)\n",
    "        test_y_err  = np.count_nonzero(test_y_pred  == test_y ) / len(test_y)\n",
    "\n",
    "        return train_y_err, test_y_err\n",
    "    \n",
    "    def run(self, args):\n",
    "        train_x, train_y = args[\"train_x\"], args[\"train_y\"]\n",
    "        k_fold = 4\n",
    "        k_indices = self._build_k_indices(len(train_y))\n",
    "        print(\"Cross Validation:\")\n",
    "        for k in range(k_fold):\n",
    "            train_err, test_err = self._cross_validation(train_y, train_x, k_indices, k)\n",
    "            print(\"#%s fold: (Train error: %s, Test error: %s)\" % (k, train_err, test_err))\n",
    "            \n",
    "class TrainPredictorPhase(BasePhase):\n",
    "    \"\"\"Trains a predictor with the given train set\"\"\"\n",
    "    def __init__(self, gamma, max_iters, lambda_ = 0):\n",
    "        self.gamma = gamma\n",
    "        self.max_iters = max_iters\n",
    "        self.lambda_ = lambda_\n",
    "        \n",
    "    def run(self, args):\n",
    "        train_x, train_y = args[\"train_x\"], args[\"train_y\"]\n",
    "        self.debug(\"Training predictor with\", train_x.shape[0], \"samples and\", train_x.shape[1], \"features.\", \n",
    "                   \"(max_iters=%s, gamma=%s, lambda=%s)\" % (self.max_iters, self.gamma, self.lambda_))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        w, _ = reg_logistic_regression(train_y, train_x, np.zeros(train_x.shape[1]), \n",
    "                                   self.max_iters, self.gamma, self.lambda_)\n",
    "        self.debug(\"Done in\", time.time() - start_time, \"seconds\")\n",
    "        return {\"pred\": Predictor(w)}\n",
    "    \n",
    "class Predictor(object):\n",
    "    def __init__(self, w):\n",
    "        self.w = w\n",
    "    \n",
    "    def predict(self, x):\n",
    "        y_pred = expit(x @ self.w)\n",
    "\n",
    "        y_pred[y_pred <  0.5] = 0\n",
    "        y_pred[y_pred >= 0.5] = 1\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "trainPredictPhase = TrainPredictorPhase(0.1, 200)\n",
    "\n",
    "train_pipeline = [\n",
    "    CrossValidationPhase(4, [trainPredictPhase]),\n",
    "    #TrainPredictorPhase(0.1, 200)\n",
    "]\n",
    "\n",
    "test_train_pipeline = [\n",
    "    Foreach([\"train_x\", \"train_y\"], train_pipeline)\n",
    "]\n",
    "\n",
    "run_pipeline(test_train_pipeline, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_train_error = 0\n",
    "total_train_samples = 0\n",
    "\n",
    "class PredictionOnTrainSetPhase(BasePhase):\n",
    "    \"\"\"Predicts the output of the given dataset\"\"\"\n",
    "    def run(self, args):\n",
    "        train_x, train_y, pred = args[\"train_x\"], args[\"train_y\"], args[\"pred\"]\n",
    "        y_pred = pred.predict(train_x)\n",
    "        global total_train_error\n",
    "        global total_train_samples\n",
    "        total_train_error += np.count_nonzero(y_pred == train_y)\n",
    "        total_train_samples += len(train_y)\n",
    "        print(np.count_nonzero(y_pred == train_y) / len(train_y), \" for \", len(train_y), \" points\")\n",
    "\n",
    "class PredictionOnTestSetPhase(BasePhase):\n",
    "    def run(self, args):\n",
    "        test_x, pred = args[\"test_x\"], args[\"pred\"]\n",
    "        y_pred = pred.predict(test_x)\n",
    "        return {\"y_pred\": y_pred}\n",
    "        \n",
    "class AggregateBackPredictionsFrom999SplitPhase(BasePhase):\n",
    "    \"\"\"Aggregates predictions\"\"\"\n",
    "    def run(self, args):\n",
    "        y_preds, row_indices = args[\"y_pred\"], args[\"test_row_indices\"]\n",
    "        \n",
    "        ret = {}\n",
    "        \n",
    "        total = 0\n",
    "        for i in range(len(row_indices)):\n",
    "            total += len(row_indices[i])\n",
    "            \n",
    "        y_pred = np.zeros(total)\n",
    "        for i in range(len(row_indices)):\n",
    "            y_pred[row_indices[i]] = y_preds[i]\n",
    "            \n",
    "        return {\"y_pred\": y_pred}\n",
    "        \n",
    "class CreateSubmissionPhase(BasePhase):\n",
    "    \"\"\"Creates the CSV submission file from the given predictions\"\"\"\n",
    "    def run(self, args):\n",
    "        y_pred, ids_test = args[\"y_pred\"], args[\"ids_test\"]\n",
    "        create_csv_submission(ids_test, y_pred * 2 - 1, OUTPUT_PATH)\n",
    "        \n",
    "test_pred_pipeline = [\n",
    "    Foreach([\"train_x\", \"train_y\", \"test_x\", \"pred\"], [\n",
    "        PredictionOnTrainSetPhase(),\n",
    "        PredictionOnTestSetPhase()\n",
    "    ]),\n",
    "    AggregateBackPredictionsFrom999SplitPhase(),\n",
    "    CreateSubmissionPhase()\n",
    "]\n",
    "\n",
    "run_pipeline(test_pred_pipeline, state)\n",
    "\n",
    "print(\"Total: \", total_train_error / total_train_samples)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
